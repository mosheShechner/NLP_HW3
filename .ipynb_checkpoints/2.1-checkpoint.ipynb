{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2.1: Random PCFG Generation\n",
    "\n",
    "In this question, we implement the function pcfg_generate(grammar) which will generate a random tree from a PCFG according to its generative probability model. In other words, we implement a sample method from the PCFG distribution. \n",
    "We use a recursive method: Given a symbol, we randomly choose a production from the that symbol. The base case is a Terminal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import PCFG, ProbabilisticProduction, MLEProbDist, FreqDist\n",
    "from nltk.grammar import Nonterminal, Production\n",
    "from nltk import probability\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def getProbDist(productions):\n",
    "    dict = {}\n",
    "    for p in productions:\n",
    "        dict[p.rhs()]=p.prob()\n",
    "    return probability.DictionaryProbDist(dict)\n",
    "\n",
    "\n",
    "def get_random_production_helper(nt,g):\n",
    "    if not isinstance(nt, Nonterminal):\n",
    "        return \"\"\n",
    "    else:\n",
    "        productions = g.productions(nt)\n",
    "        p = getProbDist(productions)\n",
    "        t = p.generate()\n",
    "        res = \"\"\n",
    "        for x in t:\n",
    "            if isinstance(x,Nonterminal):\n",
    "                res =  res + \" (\" + str(x) + \" \" + get_random_production_helper(x, g) + \")\"\n",
    "            else:\n",
    "                res = res + str(x)\n",
    "        return res\n",
    "\n",
    "\n",
    "def pcfg_generate(g):\n",
    "    res = get_random_production_helper(g.start(),g)\n",
    "    return \"(S\" + res + \")\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test our code with some example grammer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S (NP  (Det the) (N man)) (VP  (V ate) (NP  (Det the) (N telescope))))\n"
     ]
    }
   ],
   "source": [
    "g = PCFG.fromstring(\"\"\"\n",
    "    S -> NP VP [1.0]\n",
    "    NP -> Det N [0.5] | NP PP [0.25] | 'John' [0.1] | 'I' [0.15]\n",
    "    Det -> 'the' [0.8] | 'my' [0.2]\n",
    "    N -> 'man' [0.5] | 'telescope' [0.5]\n",
    "    VP -> VP PP [0.1] | V NP [0.7] | V [0.2]\n",
    "    V -> 'ate' [0.35] | 'saw' [0.65]\n",
    "    PP -> P NP [1.0]\n",
    "    P -> 'with' [0.61] | 'under' [0.39]\n",
    "    \"\"\")\n",
    "\n",
    "print(pcfg_generate(g))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we want to validate the generate function.\n",
    "\n",
    "## 2.1.1\n",
    "We generate 1,000 random trees using nltk.grammar.toy_pcfg2 - and store the resulting trees in a file \"toy_pcfg2.gen\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.grammar import toy_pcfg2\n",
    "\n",
    "\n",
    "def generate_corpus():\n",
    "    f = open(\"toy_pcfg2.gen\", \"w+\")\n",
    "    for i in range(1000):\n",
    "        s = pcfg_generate(toy_pcfg2)\n",
    "        f.write(s + \"\\r\\n\")\n",
    "    f.close()\n",
    "\n",
    "generate_corpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check some of the generated trees:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S (NP  (NP  (Det the) (N telescope)) (PP  (P with) (NP  (Name Bob)))) (VP  (V ran) (NP  (Det a) (N cookie))))\n",
      "\n",
      "(S (NP  (Det a) (N cookie)) (VP  (V ate)))\n",
      "\n",
      "(S (NP  (Det the) (N telescope)) (VP  (V ate) (NP  (NP  (Det the) (N hill)) (PP  (P under) (NP  (NP  (NP  (Name Bob)) (PP  (P with) (NP  (NP  (Det a) (N boy)) (PP  (P with) (NP  (Det the) (N boy)))))) (PP  (P with) (NP  (Det a) (N hill))))))))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file = open(\"toy_pcfg2.gen\", \"r\")\n",
    "lines = file.readlines()\n",
    "for i in range(5):\n",
    "    if lines[i] != \"\\n\":\n",
    "        print(lines[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.2 \n",
    "Now, we compute the frequency distribution of each non-terminal and pre-terminal in the generated corpus. We construct one distribution per non-terminal. First, we initialize 2 global dictionaries. one counts the number of times a given lhs occurs, and the other counts the number of times rhs occurs. Then we transform the string to tree, using the method create_tree, scan it, and update the dictionaries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_tree(sent):\n",
    "    items = re.findall(r\"\\(|\\)|\\w+\", sent)\n",
    "    return tree_helper(1,items)[0]\n",
    "\n",
    "\n",
    "def tree_helper(index,items):\n",
    "    result = []\n",
    "    item = items[index]\n",
    "    while item != \")\":\n",
    "        if item == \"(\":\n",
    "            subtree, index = tree_helper(index + 1,items)\n",
    "            result.append(subtree)\n",
    "        else:\n",
    "            result.append(item)\n",
    "        index += 1\n",
    "        item = items[index]\n",
    "    return result, index\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def leaf(t):\n",
    "    if len(t) != 2:\n",
    "        return False\n",
    "    for x in t:\n",
    "        if isinstance(x, list):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def empirical_pcdg_helper(t,lcount,pcount):\n",
    "    if leaf(t):\n",
    "        nt = Nonterminal(t[0])\n",
    "        lcount[nt] = lcount.get(nt,0)+1\n",
    "        prod = Production(nt, [t[1]])\n",
    "        pcount[prod] = pcount.get(prod,0) + 1\n",
    "    else:\n",
    "        left = Nonterminal(t[0])\n",
    "        if len(t) == 2:\n",
    "            right = Nonterminal(t[1][0])\n",
    "            prod = Production(left,[right])\n",
    "            lcount[left] = lcount.get(left, 0) + 1\n",
    "            pcount[prod] = pcount.get(prod, 0) + 1\n",
    "            empirical_pcdg_helper(t[1],lcount,pcount)\n",
    "        else:\n",
    "            right1 = Nonterminal(t[1][0])\n",
    "            right2 = Nonterminal(t[2][0])\n",
    "            lcount[left] = lcount.get(left, 0) + 1\n",
    "            prod = Production(left,[right1,right2])\n",
    "            pcount[prod] = pcount.get(prod, 0) + 1\n",
    "            empirical_pcdg_helper(t[1],lcount,pcount)\n",
    "            empirical_pcdg_helper(t[2], lcount, pcount)\n",
    "\n",
    "\n",
    "def get_empirical_pcfg():\n",
    "    global lcount, pcfg\n",
    "    lcount = {}\n",
    "    pcount = {}\n",
    "    file = open(\"toy_pcfg2.gen\", \"r\")\n",
    "    lines = file.readlines()\n",
    "    for s in lines:\n",
    "        if s != \"\\n\":\n",
    "            t = create_tree(s)\n",
    "            empirical_pcdg_helper(t, lcount, pcount)\n",
    "    prods = [\n",
    "        ProbabilisticProduction(p.lhs(), p.rhs(), prob=pcount[p] / lcount[p.lhs()])\n",
    "        for p in pcount]\n",
    "    pcfg = PCFG(toy_pcfg2.start(), prods)\n",
    "    return pcfg\n",
    "\n",
    "\n",
    "\n",
    "def getFreqDist(productions):\n",
    "    dict = {}\n",
    "    for p in productions:\n",
    "        dict[p.rhs()]=p.prob()\n",
    "    return FreqDist(dict)\n",
    "\n",
    "\n",
    "pcfg = get_empirical_pcfg()\n",
    "pcfg_nt = []\n",
    "for p in pcfg.productions():\n",
    "    if p.lhs() not in pcfg_nt:\n",
    "        pcfg_nt.append(p.lhs())\n",
    "dist_dict = {}\n",
    "for nt in pcfg_nt:\n",
    "    nt_prods = pcfg.productions(nt)\n",
    "    fd = getFreqDist(nt_prods)\n",
    "    dist_dict[nt] = fd\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dist_dict contains a distribution for each Non-Terminal. for example, let's check dist_dict[NP]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([(NP, PP), (Det, N), (Name,)])\n",
      "dict_values([0.31912964641885766, 0.40072529465095197, 0.28014505893019037])\n"
     ]
    }
   ],
   "source": [
    "dist = dist_dict[Nonterminal(\"NP\")]\n",
    "print(dist.keys())\n",
    "print(dist.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we compare it to the real NP distribution, we can see that it is a good estimation:\n",
    "\n",
    "    NP   -> NP PP         [.31]\n",
    "\n",
    "    NP   -> Det N         [.41]\n",
    "    \n",
    "    NP   -> Name          [.28]\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.3\n",
    "Now, for each distribution, we want to compute the KL-divergence between the MLE estimation of the probability distribution constructed on the test corpus and toy_pcfg2. The MLE estimation is obtained by applying the MLEProbDist estimator to the observed empirical frequency distribution. The KL-divergence is a measure of how one probability distribution is different from a another distribution, where  KL divergence of 0 indicates that the two distributions are identical. It is given by:\n",
    "\n",
    "$$\n",
    "D_\\text{KL}(P \\parallel Q) = \\sum_{x\\in\\mathcal{X}} P(x) \\log\\left(\\frac{P(x)}{Q(x)}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S: 0.0\n",
      "VP: 0.00022778125911425244\n",
      "NP: 0.0002384032349946833\n",
      "PP: 0.0\n",
      "V: 0.002103657939921455\n",
      "N: 0.0018825024734908987\n",
      "Name: 1.0280520433252473e-07\n",
      "P: 0.0009537747233433827\n",
      "Det: 0.0002967290271206069\n"
     ]
    }
   ],
   "source": [
    "non_tr = []\n",
    "for p in toy_pcfg2.productions():\n",
    "    if not p.lhs() in non_tr:\n",
    "        non_tr.append(p.lhs())\n",
    "\n",
    "\n",
    "\n",
    "def get_kl(p, q):\n",
    "    p = np.asarray(p)\n",
    "    q = np.asarray(q)\n",
    "\n",
    "    return np.sum(p * np.log(p / q))\n",
    "\n",
    "\n",
    "\n",
    "for nt in non_tr:\n",
    "    mle = MLEProbDist(dist_dict[nt])\n",
    "    prod_nt = toy_pcfg2.productions(nt)\n",
    "    t_dist = []\n",
    "    mle_dist = []\n",
    "    for pr in prod_nt:\n",
    "        t_dist.append(pr.prob())\n",
    "        mle_dist.append(mle.prob(pr.rhs()))\n",
    "    kl = get_kl(t_dist,mle_dist)\n",
    "    print(str(nt) + ': ' + str(kl))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.4 \n",
    "\n",
    "According to the results, the MLE estimation of the test corpus is very similar to the distribution. \n",
    "Since the toy_pcfg2 is a small grammer (about 30 rules), 1000 trees are good enough. In some cases we got that the KL divergence equals 0, as in the case of the nonterminal \"S\". This may seem too accurate, but this result make sense, since the true grammer toy_pcfg2 contains only one production for S: \n",
    "S    -> NP VP [1.0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
